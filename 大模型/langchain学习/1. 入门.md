```toc
```

## 基本使用

这里使用硅基流动的代码试试联通性

```python
import requests  
  
url = "https://api.siliconflow.cn/v1/chat/completions"  
API_KEY='??'

payload = {  
    "model": "Qwen/QwQ-32B",  
    "messages": [  
        {  
            "role": "user",  
            "content": "帮我写一篇关于AI的技术文章，100个字内"  
        }  
    ],  
    "stream": False,  
    "max_tokens": 512,  
    "stop": None,  
    "temperature": 0.7,  
    "top_p": 0.7,  
    "top_k": 50,  
    "frequency_penalty": 0.5,  
    "n": 1,  
    "response_format": {"type": "text"},  
    "tools": [  
        {  
            "type": "function",  
            "function": {  
                "description": "<string>",  
                "name": "<string>",  
                "parameters": {},  
                "strict": False  
            }  
        }  
    ]  
}  
headers = {  
    "Authorization": f"Bearer {API_KEY}",  
    "Content-Type": "application/json"  
}  
  
response = requests.post(url, json=payload, headers=headers)  
print(response.text)
```

## 使用 langchain 自定义大模型

```python
  
from langchain.llms.base import LLM  
from langchain_community.llms.utils import enforce_stop_tokens  
import requests  
  
API_KEY = "??"  
BASE_URL = "https://api.siliconflow.cn/v1/chat/completions"  
  
class SiliconFlow(LLM):  
    def __init__(self):  
        super().__init__()  
  
    @property  
    def _llm_type(self) -> str:  
        return "siliconflow"  
  
    def siliconflow_completions(self, model: str, prompt: str) -> str:  
        payload = {  
            "model": model,  
            "messages": [{'role':'user', 'content': prompt}],  
            'stream': False  
        }  
        headers = {  
            "Content-Type": "application/json",  
            "Authorization": f"Bearer {API_KEY}"  
        }  
        response = requests.post(BASE_URL, json=payload, headers=headers)  
        response.raise_for_status()  
        return response.json()["choices"][0]["message"]['content']  
  
    def _call(self, prompt: str, stop: list = None, model: str = 'Qwen/QwQ-32B', **kwargs) -> str:  
        response = self.siliconflow_completions(model=model, prompt=prompt)  
        if stop is not None:  
            response = enforce_stop_tokens(response, stop)  
        return response  
  
if __name__ == "__main__":  
    llm = SiliconFlow()  
    response = llm._call(prompt="帮我写一篇关于AI的技术文章，100个字内", model='Qwen/QwQ-32B')  
    print(response)
```

这里使用的是 3.13.2 版本，然后需要在 pycharm 设置中 `Python Interpreter` 中安装 langchain 和 langchain_community 两个包。

## 使用 openai 的模板

```python
from langchain_core.prompts import ChatPromptTemplate  
from langchain_openai import ChatOpenAI  
  
API_KEY = '??'  
  
llm = ChatOpenAI(api_key=API_KEY)  
  
prompt = ChatPromptTemplate([  
    ('system', '你是世界级的技术专家'),  
    ('user', '{input}')  
])  
  
chain = prompt | llm  
  
result = chain.invoke({'input': '帮我写一篇关于AI的技术文章，100字内'})  
print(result)
```

注意：这里 prompt 的创建方式推荐使用如下

```python
prompt = ChatPromptTemplate.from_messages([
    ('system', '你是世界级的技术专家'),
    ('user', '{input}')
])
```

但是这里由于 openai 访问不了就先不管了。换硅基流动

```python
from openai import OpenAI  
from langchain_core.prompts import ChatPromptTemplate  
  
API_KEY = "??"  
BASE_URL = "https://api.siliconflow.cn/v1"  
  
  
class CustomerLLM_SiliconFlow:  
    def __call__(self, prompt: str) -> str:  
        client = OpenAI(api_key=API_KEY, base_url=BASE_URL)  
  
        '''  
        一定注意：  
        当你设置了基础 URL 为 "https://api.siliconflow.cn/v1" 时，  
        客户端库会自动处理后续的路径（如 /chat/completions）  
        '''        
        response = client.chat.completions.create(  
            model = "Qwen/QwQ-32B",  
            messages = [  
                {  
                    'role': 'user',  
                    'content': f'{prompt}'  
                }  
            ]  
        )  
  
        print('-----', response)  
  
        content = ''  
        if hasattr(response, 'choices') and response.choices:  
            for choice in response.choices:  
                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):  
                    chunk_content = choice.message.content  
                    content += chunk_content  
        else:  
            raise ValueError('Unexpected response structure')  
  
        return content  
  
  
if __name__ == '__main__':  
    llm = CustomerLLM_SiliconFlow()  
  
    country = '中国'  
    country_template = '''  
    任务: 输入一个国家，输出国家的首都  
    语言: 中文  
        按json格式输出，输出格式如下:  
    country_name    capital_name    国家: {country_name}  
    '''  
    prompt_template = ChatPromptTemplate.from_template(country_template)  
    messages: list = prompt_template.format_messages(country_name=country)  
    print(messages)  
    response = llm(messages[0].content)  
    print(response)
    
```

一定注意 url 的问题，不然很容易报错 404。

一般输出都是 md 格式，可以使用转换器转换成 json 格式

```python
import langchain.chains.summarize.refine_prompts  
import langchain.output_parsers  
from langchain.output_parsers import StructuredOutputParser  
from openai import OpenAI  
from langchain_core.prompts import ChatPromptTemplate  
from langchain.output_parsers import ResponseSchema  
  
API_KEY = "??"  
BASE_URL = "https://api.siliconflow.cn/v1"  
  
  
class CustomerLLM_SiliconFlow:  
    def __call__(self, prompt: str) -> str:  
        client = OpenAI(api_key=API_KEY, base_url=BASE_URL)  
  
        '''  
        一定注意：  
        当你设置了基础 URL 为 "https://api.siliconflow.cn/v1" 时，  
        客户端库会自动处理后续的路径（如 /chat/completions）  
        '''        response = client.chat.completions.create(  
            model = "Qwen/QwQ-32B",  
            messages = [  
                {  
                    'role': 'user',  
                    'content': f'{prompt}'  
                }  
            ]  
        )  
  
        print('-----', response)  
  
        content = ''  
        if hasattr(response, 'choices') and response.choices:  
            for choice in response.choices:  
                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):  
                    chunk_content = choice.message.content  
                    content += chunk_content  
        else:  
            raise ValueError('Unexpected response structure')  
  
        return content  
  
  
if __name__ == '__main__':  
    llm = CustomerLLM_SiliconFlow()  
  
    country_schema = ResponseSchema(name='country_name', description='国家名称')  
    capital_schema = ResponseSchema(name='capital_name', description='对应国家首都')  
  
    response_schema = [country_schema, capital_schema]  
    outer_parser = StructuredOutputParser.from_response_schemas(response_schema)  
    format_instructions = outer_parser.get_format_instructions()  
  
    country_template = '''  
    任务: 输入一个国家，输出国家的首都  
    语言: 中文  
        国家: {country_name}  
    {format_instructions}    '''    prompt_template = ChatPromptTemplate.from_template(country_template)  
    messages = prompt_template.format_messages(country_name='中国',  
                                               format_instructions=format_instructions)  
  
    response = llm(messages[0].content)  
    print(response)  
  
    output: dict = outer_parser.parse(response)  
    print(output)
    
```



